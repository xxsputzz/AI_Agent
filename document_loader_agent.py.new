import os
import time
import uuid
import json
import requests
from sentence_transformers import SentenceTransformer
from chromadb import Client, Settings
from chromadb.utils import embedding_functions

# Constants
DOCUMENTS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "documents")
CHROMA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chroma_db")
MAX_CHUNKS_CONTEXT = 3
USER_ID = "default_user"
MODEL_NAME = "tinyllama:latest"  # The Ollama model to use

# Initialize SentenceTransformer model for embeddings
try:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("✓ Successfully loaded SentenceTransformer model")
except Exception as e:
    print(f"✗ Error loading SentenceTransformer model: {e}")
    model = None

# Initialize ChromaDB
try:
    client = Client(Settings(persist_directory=CHROMA_DIR))
    print(f"✓ Connected to ChromaDB at {CHROMA_DIR}")
except Exception as e:
    print(f"✗ Error connecting to ChromaDB: {e}")
    client = None

# Get or create collections
try:
    doc_collection = client.get_or_create_collection(
        name="documents",
        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')
    )
    
    history_collection = client.get_or_create_collection(
        name="conversation_history",
        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')
    )
except Exception as e:
    print(f"✗ Error creating collections: {e}")
    doc_collection = None
    history_collection = None

def get_ollama_response(prompt, model_name=MODEL_NAME):
    """Get response from Ollama API"""
    try:
        # Check if Ollama is available
        version_response = requests.get("http://localhost:11434/api/version", timeout=3)
        if version_response.status_code != 200:
            return None, "Ollama service is not available"
        
        # Get available models
        models_response = requests.get("http://localhost:11434/api/tags", timeout=3)
        if models_response.status_code != 200:
            return None, "Failed to get available models from Ollama"
        
        available_models = models_response.json().get("models", [])
        model_names = [model["name"] for model in available_models] if available_models else []
        
        # Check if our model is available or find an alternative
        if model_name not in model_names and not any(name.startswith("tinyllama") for name in model_names):
            if not model_names:
                return None, "No models available in Ollama"
            model_name = model_names[0]
            print(f"TinyLlama not found, using {model_name} instead")
        elif model_name not in model_names:
            # Find a TinyLlama variant
            for name in model_names:
                if name.startswith("tinyllama"):
                    model_name = name
                    break
        
        print(f"Using Ollama model: {model_name}")
        
        # Check if it's a simple greeting
        query_lower = prompt.split("Current message from user: ")[-1].strip().lower()
        simple_greetings = ["hi", "hello", "hey", "hi there", "hello there", "good morning", "good afternoon", "good evening"]
        is_simple_greeting = any(query_lower.startswith(greeting) for greeting in simple_greetings)
        
        if is_simple_greeting:
            # For greetings, use a much simpler prompt
            prompt = "Reply with a simple greeting of 2-4 words only, like 'Hi there!' or 'Hello!'. Current message: " + query_lower
        
        # Generate response
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.5,  # Lower temperature for more focused responses
                    "top_p": 0.9,
                    "top_k": 20,
                    "num_predict": 50  # Limit the length of responses
                }
            },
            timeout=15 if is_simple_greeting else 30
        )
        
        if response.status_code == 200:
            result = response.json().get("response")
            print(f"Received response from Ollama (length: {len(result) if result else 0})")
            
            # For simple greetings, ensure response is very brief
            if is_simple_greeting and result:
                # Extract just the greeting part, limited to 4 words
                words = result.split()
                if len(words) > 4:
                    result = ' '.join(words[:4])
                if not result.endswith(('!', '.')):
                    result += '!'
            
            return result, None
        else:
            error_msg = f"Ollama returned error code: {response.status_code}"
            if response.text:
                try:
                    error_details = response.json()
                    error_msg += f" - {error_details.get('error', '')}"
                except:
                    error_msg += f" - {response.text[:100]}"
            return None, error_msg
    except requests.exceptions.Timeout:
        return None, "Request to Ollama timed out. The model might be taking too long to respond."
    except requests.exceptions.ConnectionError:
        return None, "Error connecting to Ollama. Make sure the Ollama service is running on localhost:11434."
    except Exception as e:
        return None, f"Error getting response from Ollama: {str(e)}"

def create_prompt(query, doc_chunks, doc_metadata, history_chunks):
    """Create a prompt with context from documents and conversation history"""
    # Check if this is a simple greeting
    query_lower = query.lower().strip()
    simple_greetings = ["hi", "hello", "hey", "hi there", "hello there", "good morning", "good afternoon", "good evening"]
    is_simple_greeting = any(query_lower.startswith(greeting) for greeting in simple_greetings)
    
    if is_simple_greeting:
        return f"Respond with a simple greeting of 2-4 words maximum. Current message: {query}"
    
    # For regular queries, create a more detailed prompt
    prompt = "You are a helpful AI assistant having a natural conversation. Keep responses concise and friendly.\n\n"
    
    if doc_chunks:
        prompt += "Here are some relevant document excerpts to inform your response:\n\n"
        for i, (chunk, meta) in enumerate(zip(doc_chunks, doc_metadata)):
            source = meta.get("source", "Unknown")
            prompt += f"[DOCUMENT {i+1}] From {source}:\n{chunk}\n\n"
    
    if history_chunks:
        prompt += "Recent conversation context (use for context but don't repeat):\n\n"
        for i, history in enumerate(history_chunks):
            prompt += f"[HISTORY {i+1}]:\n{history}\n\n"
    
    prompt += f"Current message from user: {query}\n\n"
    prompt += "Remember to be concise and natural in your response. If answering a question, give a brief answer and ask if they'd like to know more."
    
    return prompt

def ai_agent(query, user_id=USER_ID):
    """Main agent function that orchestrates the response pipeline"""
    print(f"Processing query: '{query}'")
    
    # Check if this is a simple greeting
    query_lower = query.lower().strip()
    simple_greetings = ["hi", "hello", "hey", "hi there", "hello there", "good morning", "good afternoon", "good evening"]
    is_simple_greeting = any(query_lower.startswith(greeting) for greeting in simple_greetings)
    
    if is_simple_greeting:
        print("Detected simple greeting, using optimized response path")
        prompt = f"Respond with a simple greeting of 2-4 words maximum. Current message: {query}"
        response, error = get_ollama_response(prompt)
        
        if error:
            print(f"Error getting response: {error}")
            return "Hi there!"  # Fallback greeting
        
        # Ensure the greeting is very brief
        words = response.split()
        if len(words) > 4:
            response = ' '.join(words[:4])
        if not response.endswith(('!', '.')):
            response += '!'
        
        return response
    
    # For non-greeting queries, use the full pipeline
    history_chunks = get_relevant_history(user_id, query)
    doc_chunks, doc_metadata = get_relevant_documents(query)
    
    prompt = create_prompt(query, doc_chunks, doc_metadata, history_chunks)
    response, error = get_ollama_response(prompt)
    
    if error:
        print(f"AI response error: {error}")
        return "I apologize, but I encountered an error while processing your request. Please try again."
    
    add_to_history(user_id, query, response)
    return response
